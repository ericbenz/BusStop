{"name":"BusStop","tagline":"","body":"### Welcome to BusStop\r\nBusStop is an iOS app developed for the final project in COSI134 *Automated Speech Recognition* at Brandeis University. BusStop is a proof-of-concept app which integrates transit routing and predictions with a voice recognition system to afford users the flexibility offered by speech-controlled apps.\r\n\r\n### Group Members\r\n* Eric Benzschawel (ericbenz@brandeis.edu)\r\n* Adam Berger (akberger@brandeis.edu)\r\n* Suzanne Blackley (sblackle@brandeis.edu)\r\n* Swini Garimella (mrg@brandeis.edu)\r\n\r\n### Download Instructions and API keys\r\n\r\n### Major Functionalities\r\nThis is a proof-of-concept application. The majority of development time was spent on API integration, learning the Swift programming language, and familiarizing ourselves with mobile app development in general. Please understand the rough edges around things.\r\n\r\nOur primary goal was exploring how speech recognition could be integrated with existing transit app functionality. As a result, we consulted apps and websites such as [NextBus.com](https://www.nextbus.com/), [Transit](https://itunes.apple.com/us/app/transit-app-real-time-tracker/id498151501?mt=8), [T-on-Time](http://www.t-on-time.com/), [Apple Maps](http://www.apple.com/ios/maps/), and [Google Maps](http://maps.google.com/).\r\n\r\nUsing the speech recognition system, we implemented features such as:\r\n* ceasing routing and location tracking\r\n* querying bus arrival times\r\n* identifying nearby bus stops\r\n\r\n### App Use\r\n1. Open the app and allow the app to track your location and access your microphone\r\n2. To use the ASR system, touch the large microphone button once\r\n\r\n### Suggested Use\r\n* Routing to a particular location in your geographic vicinity: *Route me to Brandeis University*\r\n* Finding out when nearby busses arrive at the stop closest to you: *Is the bus coming soon?*\r\n* Retrieving your ETA to destination: *When are we going to be there?*\r\n* Stopping routing and tracking when you know where you are: *Alright, I know where I am*\r\n\r\nPlease explore the ASR functionality by issuing commands addressing the four areas above. We spent a considerable amount of time tuning the ASR system, though recognition may not be as robust as some of your queries. In these events, the app may crash or display an error message when the ASR returns a low-confidence output.\r\n\r\n### Voice Recognition System - wit.ai\r\nWe used [wit.ai](https://wit.ai), a cloud-based voice recognition system to allow our iOS app to use speech recognition. Though Siri is widely associated with speech functionalities on Apple devices, it is currently not open to developers.\r\n\r\nIntegration of wit.ai was non-trivial, as we found it was truly meant for Objective-C based apps rather than Swift based ones like this. As a result, we used @aaronabentheuer's [Swift implementation](https://github.com/aaronabentheuer/wit-ios-helloworld-swift) as a springboard to our built out app.\r\n\r\nThe responses provided by wit.ai serve as the basis for the majority of the app's major functionalities.\r\n\r\n### Other consulted APIs\r\nIn addition to wit.ai, we also used the [Google Directions API](https://developers.google.com/maps/documentation/directions/?hl=en) and the [MBTA Realtime API](http://realtime.mbta.com/portal) for public transit and routing information.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}